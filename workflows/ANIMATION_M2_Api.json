{
  "1": {
    "inputs": {
      "ckpt_name": "animagine-xl-3.1.safetensors"
    },
    "class_type": "CheckpointLoaderSimple",
    "_meta": {
      "title": "Load Checkpoint"
    }
  },
  "2": {
    "inputs": {
      "text": "score_9, (masterpiece), character, sitting, legs stretched out, foreshortening, from below, waving hand, female, sunglasses, crop top, shorts, long legs, prop, crumpled object, clear anatomy, accurate anatomy, well-defined facial features, clean hands, clean feet, clean legs, clean torso, clean hips, clean shoulders, solid black lines, continuous and smooth lines, unbroken linework, single clean stroke, bold line weight, thick outline, solid ink strokes, pure white background",
      "clip": [
        "1",
        1
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIP Text Encode (Prompt)"
    }
  },
  "3": {
    "inputs": {
      "text": "score_4, score_5, score_6, lowres, bad anatomy, deformed hands, extra fingers, missing limbs, blurry, compression artifacts, jpeg artifacts, (construction lines:1.3), (graphite:1.2), (sketch artifacts:1.2), (smudge:1.2), (guidelines:1.1), (color:1.2), (color fill:1.2), (flat colors:1.2), (cel shading:1.2), (dotted lines:1.4), (broken lines:1.4), (sketchy lines:1.3), (stippling:1.2), color, colored background, fills, gradients, texture",
      "clip": [
        "1",
        1
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIP Text Encode (Prompt)"
    }
  },
  "4": {
    "inputs": {
      "image": "Drive_Steering_Roughs_0023.png"
    },
    "class_type": "LoadImage",
    "_meta": {
      "title": "Load Image"
    }
  },
  "5": {
    "inputs": {
      "seed": 153563715982370,
      "steps": 40,
      "cfg": 9,
      "sampler_name": "dpmpp_2m_sde_gpu",
      "scheduler": "karras",
      "denoise": 0.7,
      "model": [
        "66",
        0
      ],
      "positive": [
        "79",
        0
      ],
      "negative": [
        "79",
        1
      ],
      "latent_image": [
        "65",
        0
      ]
    },
    "class_type": "KSampler",
    "_meta": {
      "title": "KSampler"
    }
  },
  "41": {
    "inputs": {
      "samples": [
        "55",
        0
      ],
      "vae": [
        "1",
        2
      ]
    },
    "class_type": "VAEDecode",
    "_meta": {
      "title": "VAE Decode"
    }
  },
  "42": {
    "inputs": {
      "filename_prefix": "ComfyUI",
      "images": [
        "52",
        0
      ]
    },
    "class_type": "SaveImage",
    "_meta": {
      "title": "Save Image"
    }
  },
  "52": {
    "inputs": {
      "rembg_session": [
        "53",
        0
      ],
      "image": [
        "41",
        0
      ]
    },
    "class_type": "ImageRemoveBackground+",
    "_meta": {
      "title": "ðŸ”§ Image Remove Background"
    }
  },
  "53": {
    "inputs": {
      "model": "isnet-general-use: general purpose",
      "providers": "CUDA"
    },
    "class_type": "RemBGSession+",
    "_meta": {
      "title": "ðŸ”§ RemBG Session"
    }
  },
  "54": {
    "inputs": {
      "filename_prefix": "ComfyUI",
      "images": [
        "41",
        0
      ]
    },
    "class_type": "SaveImage",
    "_meta": {
      "title": "Save Image"
    }
  },
  "55": {
    "inputs": {
      "seed": 153563715982370,
      "steps": 40,
      "cfg": 9,
      "sampler_name": "dpmpp_2m_sde_gpu",
      "scheduler": "karras",
      "denoise": 0.4,
      "model": [
        "66",
        0
      ],
      "positive": [
        "77",
        0
      ],
      "negative": [
        "76",
        0
      ],
      "latent_image": [
        "5",
        0
      ]
    },
    "class_type": "KSampler",
    "_meta": {
      "title": "KSampler"
    }
  },
  "61": {
    "inputs": {
      "control_net_name": "controlnet_xl_union_xinsir.safetensors"
    },
    "class_type": "ControlNetLoader",
    "_meta": {
      "title": "Load ControlNet Model"
    }
  },
  "62": {
    "inputs": {
      "strength": 0.5,
      "start_percent": 0,
      "end_percent": 0.6,
      "positive": [
        "2",
        0
      ],
      "negative": [
        "3",
        0
      ],
      "control_net": [
        "61",
        0
      ],
      "image": [
        "4",
        0
      ],
      "vae": [
        "1",
        2
      ]
    },
    "class_type": "ControlNetApplyAdvanced",
    "_meta": {
      "title": "Apply ControlNet"
    }
  },
  "65": {
    "inputs": {
      "pixels": [
        "4",
        0
      ],
      "vae": [
        "1",
        2
      ]
    },
    "class_type": "VAEEncode",
    "_meta": {
      "title": "VAE Encode"
    }
  },
  "66": {
    "inputs": {
      "weight": 0.6,
      "weight_type": "linear",
      "combine_embeds": "concat",
      "start_at": 0,
      "end_at": 0.8,
      "embeds_scaling": "V only",
      "model": [
        "1",
        0
      ],
      "ipadapter": [
        "68",
        0
      ],
      "image": [
        "87",
        0
      ],
      "clip_vision": [
        "69",
        0
      ]
    },
    "class_type": "IPAdapterAdvanced",
    "_meta": {
      "title": "IPAdapter Advanced"
    }
  },
  "68": {
    "inputs": {
      "ipadapter_file": "ip-adapter-plus_sdxl_vit-h.safetensors"
    },
    "class_type": "IPAdapterModelLoader",
    "_meta": {
      "title": "IPAdapter Model Loader"
    }
  },
  "69": {
    "inputs": {
      "clip_name": "CLIP-ViT-H-14-laion2B-s32b-b79k.safetensors"
    },
    "class_type": "CLIPVisionLoader",
    "_meta": {
      "title": "Load CLIP Vision"
    }
  },
  "72": {
    "inputs": {
      "image": "Gemini_Generated_Image_bf3h1obf3h1obf3h.png"
    },
    "class_type": "LoadImage",
    "_meta": {
      "title": "Load Image"
    }
  },
  "73": {
    "inputs": {
      "samples": [
        "5",
        0
      ],
      "vae": [
        "1",
        2
      ]
    },
    "class_type": "VAEDecode",
    "_meta": {
      "title": "VAE Decode"
    }
  },
  "74": {
    "inputs": {
      "filename_prefix": "ComfyUI",
      "images": [
        "73",
        0
      ]
    },
    "class_type": "SaveImage",
    "_meta": {
      "title": "Save Image"
    }
  },
  "76": {
    "inputs": {
      "text": "(color:1.6), shading, gradients, shadows, messy lines, rough sketch, pencil texture, gray scale, background noise, realistic, 3d render, (construction lines:1.3), (graphite:1.2), (sketch artifacts:1.2), (smudge:1.2), (guidelines:1.1), (coloring:1.4), (color fill:1.4), (flat colors:1.4), (flat colour:1.4), (cel shading:1.4), (painted:1.3), (colored lines:1.3), (pastel:1.2), (grayscale:1.4), (grey:1.4), (tones:1.3), (halftone:1.3), (screen tone:1.3), (paper texture:1.3), (crosshatching:1.3), (hatching:1.3), (noise texture:1.3), (dotted lines:1.4), (broken lines:1.4), (sketchy lines:1.3), (stippling:1.2), color, colored background, fills, texture",
      "clip": [
        "1",
        1
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIP Text Encode (Prompt)"
    }
  },
  "77": {
    "inputs": {
      "text": "(solid black lines:1.5), (vector line art:1.3), sharp edges, pure white background, digital ink, solid black lines, continuous and smooth lines, unbroken linework, single clean stroke, bold line weight, thick outline, solid ink strokes, clear anatomy, accurate anatomy, well-defined facial features, clean hands, clean feet, clean legs, clean torso, clean hips, clean shoulders",
      "clip": [
        "1",
        1
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIP Text Encode (Prompt)"
    }
  },
  "78": {
    "inputs": {
      "detect_hand": "enable",
      "detect_body": "enable",
      "detect_face": "enable",
      "resolution": 1920,
      "bbox_detector": "yolox_l.onnx",
      "pose_estimator": "dw-ll_ucoco_384_bs5.torchscript.pt",
      "scale_stick_for_xinsr_cn": "disable",
      "image": [
        "4",
        0
      ]
    },
    "class_type": "DWPreprocessor",
    "_meta": {
      "title": "DWPose Estimator"
    }
  },
  "79": {
    "inputs": {
      "strength": 1,
      "start_percent": 0,
      "end_percent": 0,
      "positive": [
        "62",
        0
      ],
      "negative": [
        "62",
        1
      ],
      "control_net": [
        "80",
        0
      ],
      "image": [
        "78",
        0
      ],
      "vae": [
        "1",
        2
      ]
    },
    "class_type": "ControlNetApplyAdvanced",
    "_meta": {
      "title": "Apply ControlNet"
    }
  },
  "80": {
    "inputs": {
      "control_net_name": "controlnet_openpose_sdxl.safetensors"
    },
    "class_type": "ControlNetLoader",
    "_meta": {
      "title": "Load ControlNet Model"
    }
  },
  "87": {
    "inputs": {
      "interpolation": "BICUBIC",
      "crop_position": "pad",
      "sharpening": 0,
      "image": [
        "72",
        0
      ]
    },
    "class_type": "PrepImageForClipVision",
    "_meta": {
      "title": "Prep Image For ClipVision"
    }
  }
}
